#!/usr/bin/env python3
"""
AI QA Agent - Phase 4 Implementation
Performs comprehensive quality assurance on code generated by AI Developer Agent
"""

import json
import os
import sys
import subprocess
import ast
import re
import logging
from pathlib import Path
from datetime import datetime
from typing import Dict, List, Any, Optional, Tuple
from dataclasses import dataclass, field, asdict
from enum import Enum
import uuid
import tempfile
import shutil

# Setup logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger('AIQAAgent')


class TestType(Enum):
    """Types of tests to perform"""
    UNIT = "unit"
    INTEGRATION = "integration"
    SECURITY = "security"
    PERFORMANCE = "performance"
    COMPLIANCE = "compliance"
    ACCESSIBILITY = "accessibility"
    E2E = "end-to-end"


class QualityLevel(Enum):
    """Quality assessment levels"""
    EXCELLENT = "excellent"
    GOOD = "good"
    ACCEPTABLE = "acceptable"
    NEEDS_IMPROVEMENT = "needs_improvement"
    POOR = "poor"


@dataclass
class CodeProject:
    """Represents a code project to test"""
    project_id: str
    project_path: str
    language: str
    framework: str
    generated_by: str
    spec_id: str
    files: List[str]
    test_files: List[str]


@dataclass
class TestResult:
    """Result of a single test"""
    test_id: str
    test_type: TestType
    test_name: str
    status: str  # passed, failed, skipped
    message: str
    duration: float
    details: Dict[str, Any] = field(default_factory=dict)


@dataclass
class SecurityIssue:
    """Security vulnerability found"""
    severity: str  # critical, high, medium, low
    type: str
    file: str
    line: int
    description: str
    recommendation: str


@dataclass
class PerformanceMetric:
    """Performance measurement"""
    metric_name: str
    value: float
    unit: str
    threshold: Optional[float] = None
    passed: bool = True


@dataclass
class QAReport:
    """Complete QA report for a project"""
    report_id: str
    project_id: str
    timestamp: str
    test_results: List[TestResult]
    security_issues: List[SecurityIssue]
    performance_metrics: List[PerformanceMetric]
    code_coverage: float
    quality_score: float
    quality_level: QualityLevel
    recommendations: List[str]
    passed: bool
    
    def to_dict(self):
        """Convert to dictionary"""
        data = asdict(self)
        data['quality_level'] = self.quality_level.value
        return data


class TestRunner:
    """Runs various types of tests on code"""
    
    def __init__(self, project: CodeProject):
        self.project = project
        self.test_results: List[TestResult] = []
    
    def run_unit_tests(self) -> List[TestResult]:
        """Run unit tests based on language/framework"""
        results = []
        
        if self.project.language.lower() == 'python':
            results = self._run_python_tests()
        elif self.project.language.lower() in ['typescript', 'javascript']:
            results = self._run_javascript_tests()
        elif self.project.language.lower() == 'go':
            results = self._run_go_tests()
        elif self.project.language.lower() == 'java':
            results = self._run_java_tests()
        
        return results
    
    def _run_python_tests(self) -> List[TestResult]:
        """Run Python tests using pytest"""
        results = []
        
        try:
            # Check if pytest is available
            project_path = Path(self.project.project_path)
            
            # Create a temporary test runner script
            test_script = project_path / "run_tests.py"
            test_content = '''
import sys
import json
import time
from pathlib import Path

# Simulate test execution
test_results = []

# Find test files
test_dir = Path("tests")
if test_dir.exists():
    test_files = list(test_dir.glob("test_*.py"))
    
    for test_file in test_files:
        # Simulate running each test file
        start_time = time.time()
        
        # Mock test results (in production, would actually run tests)
        test_results.append({
            "file": str(test_file),
            "tests": 5,
            "passed": 4,
            "failed": 1,
            "duration": time.time() - start_time
        })

print(json.dumps(test_results))
'''
            
            with open(test_script, 'w') as f:
                f.write(test_content)
            
            # Run the test script
            result = subprocess.run(
                [sys.executable, str(test_script)],
                cwd=project_path,
                capture_output=True,
                text=True,
                timeout=30
            )
            
            if result.returncode == 0 and result.stdout:
                test_data = json.loads(result.stdout)
                
                for test_file_result in test_data:
                    # Create test results
                    results.append(TestResult(
                        test_id=f"pytest-{uuid.uuid4().hex[:8]}",
                        test_type=TestType.UNIT,
                        test_name=test_file_result['file'],
                        status="passed" if test_file_result['failed'] == 0 else "failed",
                        message=f"{test_file_result['passed']}/{test_file_result['tests']} tests passed",
                        duration=test_file_result['duration'],
                        details=test_file_result
                    ))
            
            # Clean up
            test_script.unlink()
            
        except subprocess.TimeoutExpired:
            results.append(TestResult(
                test_id=f"pytest-timeout-{uuid.uuid4().hex[:8]}",
                test_type=TestType.UNIT,
                test_name="Python tests",
                status="failed",
                message="Test execution timed out",
                duration=30.0
            ))
        except Exception as e:
            results.append(TestResult(
                test_id=f"pytest-error-{uuid.uuid4().hex[:8]}",
                test_type=TestType.UNIT,
                test_name="Python tests",
                status="failed",
                message=f"Error running tests: {str(e)}",
                duration=0.0
            ))
        
        return results
    
    def _run_javascript_tests(self) -> List[TestResult]:
        """Run JavaScript/TypeScript tests"""
        results = []
        
        # Mock JavaScript test results
        results.append(TestResult(
            test_id=f"jest-{uuid.uuid4().hex[:8]}",
            test_type=TestType.UNIT,
            test_name="JavaScript tests",
            status="passed",
            message="All tests passed",
            duration=2.5,
            details={"framework": "jest", "tests": 10, "passed": 10}
        ))
        
        return results
    
    def _run_go_tests(self) -> List[TestResult]:
        """Run Go tests"""
        results = []
        
        # Mock Go test results
        results.append(TestResult(
            test_id=f"go-test-{uuid.uuid4().hex[:8]}",
            test_type=TestType.UNIT,
            test_name="Go tests",
            status="passed",
            message="PASS",
            duration=1.2,
            details={"packages": 3, "coverage": "85%"}
        ))
        
        return results
    
    def _run_java_tests(self) -> List[TestResult]:
        """Run Java tests"""
        results = []
        
        # Mock Java test results
        results.append(TestResult(
            test_id=f"junit-{uuid.uuid4().hex[:8]}",
            test_type=TestType.UNIT,
            test_name="Java tests",
            status="passed",
            message="Tests run: 15, Failures: 0",
            duration=3.8,
            details={"framework": "junit", "tests": 15, "passed": 15}
        ))
        
        return results
    
    def run_integration_tests(self) -> List[TestResult]:
        """Run integration tests"""
        results = []
        
        # Simulate integration testing
        results.append(TestResult(
            test_id=f"integration-{uuid.uuid4().hex[:8]}",
            test_type=TestType.INTEGRATION,
            test_name="API Integration",
            status="passed",
            message="All endpoints responding correctly",
            duration=5.2,
            details={"endpoints_tested": 8, "passed": 8}
        ))
        
        results.append(TestResult(
            test_id=f"integration-{uuid.uuid4().hex[:8]}",
            test_type=TestType.INTEGRATION,
            test_name="Database Integration",
            status="passed",
            message="Database operations working",
            duration=2.1,
            details={"operations_tested": 5, "passed": 5}
        ))
        
        return results


class SecurityScanner:
    """Scans code for security vulnerabilities"""
    
    def __init__(self, project: CodeProject):
        self.project = project
        self.security_issues: List[SecurityIssue] = []
    
    def scan_code(self) -> List[SecurityIssue]:
        """Perform security scanning"""
        issues = []
        
        # Scan based on language
        if self.project.language.lower() == 'python':
            issues.extend(self._scan_python_security())
        elif self.project.language.lower() in ['typescript', 'javascript']:
            issues.extend(self._scan_javascript_security())
        
        # Common security checks
        issues.extend(self._scan_common_vulnerabilities())
        
        return issues
    
    def _scan_python_security(self) -> List[SecurityIssue]:
        """Scan Python code for security issues"""
        issues = []
        
        project_path = Path(self.project.project_path)
        
        # Common Python security patterns to check
        security_patterns = [
            (r'eval\s*\(', 'Use of eval()', 'critical', 'Avoid using eval() as it can execute arbitrary code'),
            (r'exec\s*\(', 'Use of exec()', 'critical', 'Avoid using exec() as it can execute arbitrary code'),
            (r'pickle\.loads?\s*\(', 'Unsafe deserialization', 'high', 'Use safer serialization formats like JSON'),
            (r'os\.system\s*\(', 'Command injection risk', 'high', 'Use subprocess with proper input validation'),
            (r'subprocess.*shell\s*=\s*True', 'Shell injection risk', 'high', 'Avoid shell=True in subprocess calls'),
            (r'SECRET_KEY\s*=\s*["\'][\w]+["\']', 'Hardcoded secrets', 'high', 'Use environment variables for secrets'),
            (r'password\s*=\s*["\'][\w]+["\']', 'Hardcoded password', 'critical', 'Never hardcode passwords'),
            (r'verify\s*=\s*False', 'SSL verification disabled', 'medium', 'Always verify SSL certificates'),
        ]
        
        # Scan Python files
        for file_path in project_path.rglob('*.py'):
            try:
                with open(file_path, 'r', encoding='utf-8') as f:
                    content = f.read()
                    lines = content.splitlines()
                    
                    for pattern, issue_type, severity, recommendation in security_patterns:
                        for i, line in enumerate(lines, 1):
                            if re.search(pattern, line):
                                issues.append(SecurityIssue(
                                    severity=severity,
                                    type=issue_type,
                                    file=str(file_path.relative_to(project_path)),
                                    line=i,
                                    description=f"Found {issue_type} in code",
                                    recommendation=recommendation
                                ))
            except Exception as e:
                logger.warning(f"Error scanning {file_path}: {e}")
        
        return issues
    
    def _scan_javascript_security(self) -> List[SecurityIssue]:
        """Scan JavaScript/TypeScript code for security issues"""
        issues = []
        
        project_path = Path(self.project.project_path)
        
        # Common JavaScript security patterns
        security_patterns = [
            (r'eval\s*\(', 'Use of eval()', 'critical', 'Avoid using eval() as it can execute arbitrary code'),
            (r'innerHTML\s*=', 'XSS vulnerability', 'high', 'Use textContent or proper sanitization'),
            (r'dangerouslySetInnerHTML', 'React XSS risk', 'high', 'Sanitize content before using dangerouslySetInnerHTML'),
            (r'document\.write\s*\(', 'DOM-based XSS', 'medium', 'Use safer DOM manipulation methods'),
            (r'localStorage\.setItem.*password', 'Password in localStorage', 'critical', 'Never store passwords in localStorage'),
        ]
        
        # Scan JavaScript/TypeScript files
        for ext in ['*.js', '*.ts', '*.jsx', '*.tsx']:
            for file_path in project_path.rglob(ext):
                if 'node_modules' in str(file_path):
                    continue
                    
                try:
                    with open(file_path, 'r', encoding='utf-8') as f:
                        content = f.read()
                        lines = content.splitlines()
                        
                        for pattern, issue_type, severity, recommendation in security_patterns:
                            for i, line in enumerate(lines, 1):
                                if re.search(pattern, line):
                                    issues.append(SecurityIssue(
                                        severity=severity,
                                        type=issue_type,
                                        file=str(file_path.relative_to(project_path)),
                                        line=i,
                                        description=f"Found {issue_type} in code",
                                        recommendation=recommendation
                                    ))
                except Exception as e:
                    logger.warning(f"Error scanning {file_path}: {e}")
        
        return issues
    
    def _scan_common_vulnerabilities(self) -> List[SecurityIssue]:
        """Scan for common vulnerabilities across languages"""
        issues = []
        
        project_path = Path(self.project.project_path)
        
        # Check for sensitive files
        sensitive_files = ['.env', '.env.local', 'config.json', 'secrets.yml']
        for sensitive_file in sensitive_files:
            for file_path in project_path.rglob(sensitive_file):
                if file_path.exists():
                    issues.append(SecurityIssue(
                        severity='medium',
                        type='Sensitive file exposed',
                        file=str(file_path.relative_to(project_path)),
                        line=0,
                        description=f'Sensitive file {sensitive_file} found',
                        recommendation='Ensure sensitive files are in .gitignore'
                    ))
        
        return issues


class PerformanceAnalyzer:
    """Analyzes code performance characteristics"""
    
    def __init__(self, project: CodeProject):
        self.project = project
        self.metrics: List[PerformanceMetric] = []
    
    def analyze_performance(self) -> List[PerformanceMetric]:
        """Analyze performance characteristics"""
        metrics = []
        
        # Code complexity analysis
        metrics.extend(self._analyze_complexity())
        
        # File size analysis
        metrics.extend(self._analyze_file_sizes())
        
        # Dependency analysis
        metrics.extend(self._analyze_dependencies())
        
        return metrics
    
    def _analyze_complexity(self) -> List[PerformanceMetric]:
        """Analyze code complexity"""
        metrics = []
        
        project_path = Path(self.project.project_path)
        
        if self.project.language.lower() == 'python':
            # Analyze Python complexity
            total_complexity = 0
            file_count = 0
            
            for file_path in project_path.rglob('*.py'):
                try:
                    with open(file_path, 'r', encoding='utf-8') as f:
                        content = f.read()
                        tree = ast.parse(content)
                        
                        # Count functions and classes
                        functions = sum(1 for node in ast.walk(tree) if isinstance(node, ast.FunctionDef))
                        classes = sum(1 for node in ast.walk(tree) if isinstance(node, ast.ClassDef))
                        
                        # Simple complexity calculation
                        complexity = functions * 2 + classes * 3
                        total_complexity += complexity
                        file_count += 1
                        
                except Exception as e:
                    logger.warning(f"Error analyzing {file_path}: {e}")
            
            if file_count > 0:
                avg_complexity = total_complexity / file_count
                metrics.append(PerformanceMetric(
                    metric_name="Average Cyclomatic Complexity",
                    value=round(avg_complexity, 2),
                    unit="complexity",
                    threshold=10.0,
                    passed=avg_complexity < 10.0
                ))
        
        return metrics
    
    def _analyze_file_sizes(self) -> List[PerformanceMetric]:
        """Analyze file sizes"""
        metrics = []
        
        project_path = Path(self.project.project_path)
        
        total_size = 0
        file_count = 0
        large_files = []
        
        for file_path in project_path.rglob('*'):
            if file_path.is_file() and 'node_modules' not in str(file_path):
                size = file_path.stat().st_size
                total_size += size
                file_count += 1
                
                # Flag large files (>100KB)
                if size > 100000:
                    large_files.append((str(file_path.relative_to(project_path)), size))
        
        if file_count > 0:
            avg_size = total_size / file_count
            metrics.append(PerformanceMetric(
                metric_name="Average File Size",
                value=round(avg_size / 1024, 2),
                unit="KB",
                threshold=50.0,
                passed=avg_size < 50000
            ))
            
            metrics.append(PerformanceMetric(
                metric_name="Total Project Size",
                value=round(total_size / 1024 / 1024, 2),
                unit="MB",
                threshold=100.0,
                passed=total_size < 100 * 1024 * 1024
            ))
            
            if large_files:
                metrics.append(PerformanceMetric(
                    metric_name="Large Files Count",
                    value=len(large_files),
                    unit="files",
                    threshold=5,
                    passed=len(large_files) < 5
                ))
        
        return metrics
    
    def _analyze_dependencies(self) -> List[PerformanceMetric]:
        """Analyze project dependencies"""
        metrics = []
        
        project_path = Path(self.project.project_path)
        
        if self.project.language.lower() == 'python':
            # Check requirements.txt
            req_file = project_path / 'requirements.txt'
            if req_file.exists():
                with open(req_file, 'r') as f:
                    dependencies = [line.strip() for line in f if line.strip() and not line.startswith('#')]
                    
                metrics.append(PerformanceMetric(
                    metric_name="Python Dependencies",
                    value=len(dependencies),
                    unit="packages",
                    threshold=50,
                    passed=len(dependencies) < 50
                ))
        
        elif self.project.language.lower() in ['typescript', 'javascript']:
            # Check package.json
            package_file = project_path / 'package.json'
            if package_file.exists():
                with open(package_file, 'r') as f:
                    package_data = json.load(f)
                    deps = package_data.get('dependencies', {})
                    dev_deps = package_data.get('devDependencies', {})
                    
                    total_deps = len(deps) + len(dev_deps)
                    metrics.append(PerformanceMetric(
                        metric_name="NPM Dependencies",
                        value=total_deps,
                        unit="packages",
                        threshold=100,
                        passed=total_deps < 100
                    ))
        
        return metrics


class ComplianceChecker:
    """Checks code compliance with standards"""
    
    def __init__(self, project: CodeProject):
        self.project = project
        self.compliance_issues: List[Dict[str, Any]] = []
    
    def check_compliance(self) -> Dict[str, Any]:
        """Check various compliance standards"""
        compliance_results = {
            'licensing': self._check_licensing(),
            'documentation': self._check_documentation(),
            'code_standards': self._check_code_standards(),
            'accessibility': self._check_accessibility(),
            'gdpr': self._check_gdpr_compliance()
        }
        
        return compliance_results
    
    def _check_licensing(self) -> Dict[str, Any]:
        """Check for proper licensing"""
        project_path = Path(self.project.project_path)
        
        license_files = ['LICENSE', 'LICENSE.txt', 'LICENSE.md']
        has_license = any((project_path / f).exists() for f in license_files)
        
        return {
            'has_license': has_license,
            'recommendation': 'Add a LICENSE file' if not has_license else 'License present'
        }
    
    def _check_documentation(self) -> Dict[str, Any]:
        """Check documentation completeness"""
        project_path = Path(self.project.project_path)
        
        docs = {
            'README': any((project_path / f).exists() for f in ['README.md', 'README.txt', 'README']),
            'API_DOCS': (project_path / 'docs').exists(),
            'CONTRIBUTING': any((project_path / f).exists() for f in ['CONTRIBUTING.md', 'CONTRIBUTING.txt']),
            'CHANGELOG': any((project_path / f).exists() for f in ['CHANGELOG.md', 'CHANGELOG.txt'])
        }
        
        completeness = sum(docs.values()) / len(docs) * 100
        
        return {
            'documentation_completeness': f"{completeness:.0f}%",
            'missing': [k for k, v in docs.items() if not v],
            'recommendation': 'Add missing documentation' if completeness < 75 else 'Documentation adequate'
        }
    
    def _check_code_standards(self) -> Dict[str, Any]:
        """Check code standards compliance"""
        project_path = Path(self.project.project_path)
        
        standards = {
            'linting_config': False,
            'formatting_config': False,
            'testing_config': False
        }
        
        # Check for linting/formatting configs
        if self.project.language.lower() == 'python':
            standards['linting_config'] = any((project_path / f).exists() for f in ['.flake8', '.pylintrc', 'pyproject.toml'])
            standards['formatting_config'] = any((project_path / f).exists() for f in ['.black', 'pyproject.toml'])
            standards['testing_config'] = (project_path / 'pytest.ini').exists() or (project_path / 'pyproject.toml').exists()
        elif self.project.language.lower() in ['typescript', 'javascript']:
            standards['linting_config'] = any((project_path / f).exists() for f in ['.eslintrc', '.eslintrc.json', '.eslintrc.js'])
            standards['formatting_config'] = (project_path / '.prettierrc').exists()
            standards['testing_config'] = (project_path / 'jest.config.js').exists()
        
        compliance = sum(standards.values()) / len(standards) * 100
        
        return {
            'code_standards_compliance': f"{compliance:.0f}%",
            'standards': standards,
            'recommendation': 'Add code quality tools' if compliance < 66 else 'Code standards adequate'
        }
    
    def _check_accessibility(self) -> Dict[str, Any]:
        """Check accessibility compliance (for web projects)"""
        # This would check for WCAG compliance in web projects
        return {
            'applicable': 'web' in self.project.framework.lower(),
            'wcag_level': 'Not assessed',
            'recommendation': 'Perform accessibility audit for web components'
        }
    
    def _check_gdpr_compliance(self) -> Dict[str, Any]:
        """Check GDPR compliance indicators"""
        project_path = Path(self.project.project_path)
        
        privacy_indicators = {
            'privacy_policy': any((project_path / f).exists() for f in ['PRIVACY.md', 'privacy.txt']),
            'data_retention': False,  # Would need to analyze code
            'user_consent': False,  # Would need to analyze code
            'data_encryption': False  # Would need to analyze security features
        }
        
        return {
            'gdpr_indicators': privacy_indicators,
            'recommendation': 'Review GDPR requirements if handling EU user data'
        }


class AIQAAgent:
    """Main AI QA Agent"""
    
    def __init__(self):
        self.logger = logger
        self.test_runner = None
        self.security_scanner = None
        self.performance_analyzer = None
        self.compliance_checker = None
        
        # AWS BACKEND PROCESSING POLICY - MANDATORY COMPLIANCE
        self.aws_backend_policy = {
            "priority_order": [
                "AWS Lambda (serverless functions)",
                "AWS Fargate Tasks (Batch/Step Functions)", 
                "AWS Fargate Container Service (ECS/EKS)",
                "EC2 (requires justification)"
            ],
            "qa_validation_criteria": {
                "lambda_compliance": {
                    "timeout_handling": "Must handle 15-minute limit",
                    "memory_efficiency": "Optimize for cost",
                    "cold_start_optimization": "Required",
                    "stateless_design": "Mandatory"
                },
                "fargate_compliance": {
                    "container_optimization": "Multi-stage builds",
                    "resource_efficiency": "Right-sizing",
                    "scaling_behavior": "Auto-scale validation"
                },
                "serverless_testing": {
                    "scale_to_zero": "Test idle behavior",
                    "auto_scaling": "Load testing required",
                    "cost_optimization": "Validate usage patterns"
                }
            }
        }
        
        logger.info("AI QA Agent initialized with AWS Serverless Policy validation")
    
    def analyze_project(self, project_path: str, project_info: Dict[str, Any]) -> QAReport:
        """Perform comprehensive QA analysis on a project"""
        logger.info(f"Starting QA analysis for project: {project_path}")
        
        # Create project object
        project = CodeProject(
            project_id=project_info.get('project_id', f"proj-{uuid.uuid4().hex[:8]}"),
            project_path=project_path,
            language=project_info.get('language', 'python'),
            framework=project_info.get('framework', 'fastapi'),
            generated_by=project_info.get('generated_by', 'ai-developer'),
            spec_id=project_info.get('spec_id', 'unknown'),
            files=self._get_project_files(project_path),
            test_files=self._get_test_files(project_path)
        )
        
        # Initialize analyzers
        self.test_runner = TestRunner(project)
        self.security_scanner = SecurityScanner(project)
        self.performance_analyzer = PerformanceAnalyzer(project)
        self.compliance_checker = ComplianceChecker(project)
        
        # Run all analyses
        logger.info("Running unit tests...")
        test_results = self.test_runner.run_unit_tests()
        
        logger.info("Running integration tests...")
        test_results.extend(self.test_runner.run_integration_tests())
        
        logger.info("Performing security scan...")
        security_issues = self.security_scanner.scan_code()
        
        logger.info("Analyzing performance...")
        performance_metrics = self.performance_analyzer.analyze_performance()
        
        logger.info("Checking compliance...")
        compliance_results = self.compliance_checker.check_compliance()
        
        # Calculate code coverage (mock for now)
        code_coverage = self._calculate_code_coverage(project)
        
        # Calculate quality score
        quality_score = self._calculate_quality_score(
            test_results, security_issues, performance_metrics, compliance_results, code_coverage
        )
        
        # Determine quality level
        quality_level = self._determine_quality_level(quality_score)
        
        # Generate recommendations
        recommendations = self._generate_recommendations(
            test_results, security_issues, performance_metrics, compliance_results
        )
        
        # Determine if QA passed
        passed = self._determine_pass_status(quality_score, security_issues)
        
        # Create report
        report = QAReport(
            report_id=f"qa-{uuid.uuid4().hex[:8]}",
            project_id=project.project_id,
            timestamp=datetime.now().isoformat(),
            test_results=test_results,
            security_issues=security_issues,
            performance_metrics=performance_metrics,
            code_coverage=code_coverage,
            quality_score=quality_score,
            quality_level=quality_level,
            recommendations=recommendations,
            passed=passed
        )
        
        logger.info(f"QA analysis complete. Quality Score: {quality_score:.1f}/100 - {quality_level.value}")
        
        return report
    
    def _get_project_files(self, project_path: str) -> List[str]:
        """Get list of project files"""
        project_dir = Path(project_path)
        files = []
        
        for file_path in project_dir.rglob('*'):
            if file_path.is_file() and 'node_modules' not in str(file_path):
                files.append(str(file_path.relative_to(project_dir)))
        
        return files
    
    def _get_test_files(self, project_path: str) -> List[str]:
        """Get list of test files"""
        project_dir = Path(project_path)
        test_files = []
        
        # Python test files
        for pattern in ['test_*.py', '*_test.py']:
            test_files.extend([str(f.relative_to(project_dir)) for f in project_dir.rglob(pattern)])
        
        # JavaScript test files
        for pattern in ['*.test.js', '*.spec.js', '*.test.ts', '*.spec.ts']:
            test_files.extend([str(f.relative_to(project_dir)) for f in project_dir.rglob(pattern)])
        
        return test_files
    
    def _calculate_code_coverage(self, project: CodeProject) -> float:
        """Calculate code coverage percentage"""
        # Mock calculation based on test file ratio
        if not project.files:
            return 0.0
        
        test_ratio = len(project.test_files) / len(project.files)
        
        # Estimate coverage (this would normally run actual coverage tools)
        estimated_coverage = min(test_ratio * 200, 95.0)  # Cap at 95%
        
        return round(estimated_coverage, 1)
    
    def _calculate_quality_score(self, test_results: List[TestResult], 
                                security_issues: List[SecurityIssue],
                                performance_metrics: List[PerformanceMetric],
                                compliance_results: Dict[str, Any],
                                code_coverage: float) -> float:
        """Calculate overall quality score"""
        score = 100.0
        
        # Test results impact (30%)
        if test_results:
            passed_tests = sum(1 for t in test_results if t.status == 'passed')
            test_score = (passed_tests / len(test_results)) * 30
        else:
            test_score = 15  # Partial credit if no tests
        
        # Security impact (30%)
        security_deductions = {
            'critical': 10,
            'high': 5,
            'medium': 2,
            'low': 0.5
        }
        security_score = 30
        for issue in security_issues:
            security_score -= security_deductions.get(issue.severity, 0)
        security_score = max(0, security_score)
        
        # Performance impact (20%)
        if performance_metrics:
            passed_metrics = sum(1 for m in performance_metrics if m.passed)
            performance_score = (passed_metrics / len(performance_metrics)) * 20
        else:
            performance_score = 10
        
        # Code coverage impact (10%)
        coverage_score = (code_coverage / 100) * 10
        
        # Compliance impact (10%)
        compliance_score = 10  # Full score if basic compliance met
        
        total_score = test_score + security_score + performance_score + coverage_score + compliance_score
        
        return round(min(100, max(0, total_score)), 1)
    
    def _determine_quality_level(self, quality_score: float) -> QualityLevel:
        """Determine quality level based on score"""
        if quality_score >= 90:
            return QualityLevel.EXCELLENT
        elif quality_score >= 75:
            return QualityLevel.GOOD
        elif quality_score >= 60:
            return QualityLevel.ACCEPTABLE
        elif quality_score >= 40:
            return QualityLevel.NEEDS_IMPROVEMENT
        else:
            return QualityLevel.POOR
    
    def _generate_recommendations(self, test_results: List[TestResult],
                                 security_issues: List[SecurityIssue],
                                 performance_metrics: List[PerformanceMetric],
                                 compliance_results: Dict[str, Any]) -> List[str]:
        """Generate improvement recommendations"""
        recommendations = []
        
        # Test recommendations
        failed_tests = [t for t in test_results if t.status == 'failed']
        if failed_tests:
            recommendations.append(f"Fix {len(failed_tests)} failing tests")
        
        if not test_results:
            recommendations.append("Add comprehensive test suite")
        
        # Security recommendations
        critical_issues = [i for i in security_issues if i.severity == 'critical']
        if critical_issues:
            recommendations.append(f"Address {len(critical_issues)} critical security issues immediately")
        
        high_issues = [i for i in security_issues if i.severity == 'high']
        if high_issues:
            recommendations.append(f"Fix {len(high_issues)} high-severity security vulnerabilities")
        
        # Performance recommendations
        failed_metrics = [m for m in performance_metrics if not m.passed]
        for metric in failed_metrics:
            recommendations.append(f"Improve {metric.metric_name}: current {metric.value}{metric.unit}, target < {metric.threshold}{metric.unit}")
        
        # Compliance recommendations
        if compliance_results.get('documentation', {}).get('missing'):
            missing_docs = compliance_results['documentation']['missing']
            recommendations.append(f"Add missing documentation: {', '.join(missing_docs)}")
        
        if not compliance_results.get('licensing', {}).get('has_license'):
            recommendations.append("Add a LICENSE file to the project")
        
        # Limit recommendations
        return recommendations[:10]
    
    def _determine_pass_status(self, quality_score: float, security_issues: List[SecurityIssue]) -> bool:
        """Determine if QA passed"""
        # Fail if quality score is too low
        if quality_score < 60:
            return False
        
        # Fail if there are critical security issues
        critical_issues = [i for i in security_issues if i.severity == 'critical']
        if critical_issues:
            return False
        
        return True
    
    def generate_report_summary(self, report: QAReport) -> str:
        """Generate a human-readable report summary"""
        summary = f"""
QA Report Summary
================
Report ID: {report.report_id}
Project ID: {report.project_id}
Timestamp: {report.timestamp}

Quality Assessment
------------------
Overall Score: {report.quality_score}/100
Quality Level: {report.quality_level.value.upper()}
Status: {'PASSED' if report.passed else 'FAILED'}

Test Results
------------
Total Tests: {len(report.test_results)}
Passed: {sum(1 for t in report.test_results if t.status == 'passed')}
Failed: {sum(1 for t in report.test_results if t.status == 'failed')}
Code Coverage: {report.code_coverage}%

Security Analysis
-----------------
Critical Issues: {sum(1 for i in report.security_issues if i.severity == 'critical')}
High Issues: {sum(1 for i in report.security_issues if i.severity == 'high')}
Medium Issues: {sum(1 for i in report.security_issues if i.severity == 'medium')}
Low Issues: {sum(1 for i in report.security_issues if i.severity == 'low')}

Performance Metrics
-------------------
"""
        for metric in report.performance_metrics[:5]:
            status = "PASS" if metric.passed else "FAIL"
            summary += f"- {metric.metric_name}: {metric.value}{metric.unit} [{status}]\n"
        
        summary += f"""
Recommendations
---------------
"""
        for i, rec in enumerate(report.recommendations[:5], 1):
            summary += f"{i}. {rec}\n"
        
        return summary


def main():
    """Main entry point for testing"""
    logger.info("AI QA Agent starting...")
    
    # Test with the generated project
    qa_agent = AIQAAgent()
    
    # Analyze the test API project generated by AI Developer
    project_info = {
        'project_id': 'test-api-001',
        'language': 'python',
        'framework': 'fastapi',
        'generated_by': 'ai-developer-agent',
        'spec_id': 'spec-test-001'
    }
    
    project_path = 'generated_projects/test-api'
    
    if Path(project_path).exists():
        report = qa_agent.analyze_project(project_path, project_info)
        
        # Print summary
        print(qa_agent.generate_report_summary(report))
        
        # Save full report
        report_file = Path('qa_reports') / f"{report.report_id}.json"
        report_file.parent.mkdir(exist_ok=True)
        
        with open(report_file, 'w') as f:
            json.dump(report.to_dict(), f, indent=2, default=str)
        
        logger.info(f"Full report saved to: {report_file}")
    else:
        logger.warning(f"Project path not found: {project_path}")
        logger.info("Creating mock report for demonstration...")
        
        # Create mock report for testing
        mock_project = CodeProject(
            project_id="mock-001",
            project_path="mock_project",
            language="python",
            framework="fastapi",
            generated_by="ai-developer",
            spec_id="spec-mock-001",
            files=["main.py", "models.py", "routes.py"],
            test_files=["test_main.py", "test_models.py"]
        )
        
        report = QAReport(
            report_id=f"qa-{uuid.uuid4().hex[:8]}",
            project_id=mock_project.project_id,
            timestamp=datetime.now().isoformat(),
            test_results=[
                TestResult(
                    test_id="test-001",
                    test_type=TestType.UNIT,
                    test_name="test_main.py",
                    status="passed",
                    message="All tests passed",
                    duration=1.5
                )
            ],
            security_issues=[],
            performance_metrics=[
                PerformanceMetric(
                    metric_name="Average File Size",
                    value=15.2,
                    unit="KB",
                    threshold=50.0,
                    passed=True
                )
            ],
            code_coverage=75.5,
            quality_score=82.3,
            quality_level=QualityLevel.GOOD,
            recommendations=["Increase test coverage to >80%", "Add integration tests"],
            passed=True
        )
        
        print(qa_agent.generate_report_summary(report))


if __name__ == "__main__":
    main()